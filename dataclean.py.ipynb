{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# 1. Data cleaning"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ce2bf6db3d8f9ea9"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import pandas as pd  # Import pandas for data manipulation using dataframes\n",
    "import numpy as np   # Import numpy for numerical operations and calculations\n",
    "\n",
    "# Attempt to load the dataset from a CSV file.\n",
    "# This block tries to open the file 'sensordata.csv' which should be located in the script's running directory or a specified path.\n",
    "try:\n",
    "    data = pd.read_csv('sensordata.csv')  # Load the data using pandas' read_csv function\n",
    "    print(\"File loaded successfully.\")   # Print success message if file loads without issues\n",
    "except Exception as e:\n",
    "    print(f\"Error loading file: {e}\")    # Print an error message if the file cannot be loaded, displaying the exception\n",
    "\n",
    "# Check for and fill missing values in the dataset\n",
    "# 'voc' column is checked for missing values which are then filled with the median of the column.\n",
    "# Filling with the median (as opposed to mean) is more robust to outliers.\n",
    "data['voc'].fillna(data['voc'].median(), inplace=True)\n",
    "\n",
    "# Remove duplicate rows from the dataset\n",
    "# Duplicates might skew or duplicate analysis results, so they are removed to ensure data quality.\n",
    "data.drop_duplicates(inplace=True)\n",
    "\n",
    "# Define columns to check for outliers\n",
    "# These columns represent sensor readings and other measurements where extreme values may indicate sensor errors or anomalies.\n",
    "cols = ['humidity', 'temperature', 'pressure', 'voc', 'pm2.5_alt',\n",
    "        '0.3_um_count', '0.5_um_count', '1.0_um_count', '2.5_um_count', \n",
    "        '5.0_um_count', '10.0_um_count']\n",
    "\n",
    "# Remove rows where any sensor reading exceeds 3 standard deviations from the mean\n",
    "# This is a method of identifying and removing outliers which are statistically rare and may represent errors.\n",
    "# Using standard deviation helps in finding outlying data points that differ significantly from the average distribution.\n",
    "data_cleaned = data[(np.abs(data[cols] - data[cols].mean()) <= (3 * data[cols].std())).all(axis=1)]\n",
    "\n",
    "# Save the cleaned data to a new CSV file if needed\n",
    "# This step is crucial for documenting the cleaned dataset and using it in subsequent analysis without repeating cleaning steps.\n",
    "try:\n",
    "    data_cleaned.to_csv('sensorcleaned.csv', index=False)  # Save the cleaned data as 'sensorcleaned.csv' without the index\n",
    "    print(\"File saved successfully.\")  # Print success message if the file is saved without issues\n",
    "except Exception as e:\n",
    "    print(f\"Error saving file: {e}\")  # Print an error message if there is an issue saving the file, displaying the exception\n",
    "\n",
    "# Display the first few rows of the cleaned data to confirm the changes\n",
    "# This step is useful for a quick check to ensure that the cleaning process has been performed as expected.\n",
    "print(data_cleaned.head())"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "initial_id",
   "execution_count": 5
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 2. Prediction"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1f8c985960689ee9"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 27 candidates, totalling 81 fits\n",
      "[CV] END .max_depth=10, min_samples_split=2, n_estimators=50; total time=   0.7s\n",
      "[CV] END .max_depth=10, min_samples_split=2, n_estimators=50; total time=   0.7s\n",
      "[CV] END .max_depth=10, min_samples_split=2, n_estimators=50; total time=   0.8s\n",
      "[CV] END max_depth=10, min_samples_split=2, n_estimators=100; total time=   1.4s\n",
      "[CV] END max_depth=10, min_samples_split=2, n_estimators=100; total time=   1.4s\n",
      "[CV] END max_depth=10, min_samples_split=2, n_estimators=100; total time=   1.4s\n",
      "[CV] END max_depth=10, min_samples_split=2, n_estimators=150; total time=   2.1s\n",
      "[CV] END max_depth=10, min_samples_split=2, n_estimators=150; total time=   2.1s\n",
      "[CV] END max_depth=10, min_samples_split=2, n_estimators=150; total time=   2.1s\n",
      "[CV] END .max_depth=10, min_samples_split=5, n_estimators=50; total time=   0.7s\n",
      "[CV] END .max_depth=10, min_samples_split=5, n_estimators=50; total time=   0.7s\n",
      "[CV] END .max_depth=10, min_samples_split=5, n_estimators=50; total time=   0.7s\n",
      "[CV] END max_depth=10, min_samples_split=5, n_estimators=100; total time=   1.4s\n",
      "[CV] END max_depth=10, min_samples_split=5, n_estimators=100; total time=   1.4s\n",
      "[CV] END max_depth=10, min_samples_split=5, n_estimators=100; total time=   1.4s\n",
      "[CV] END max_depth=10, min_samples_split=5, n_estimators=150; total time=   2.2s\n",
      "[CV] END max_depth=10, min_samples_split=5, n_estimators=150; total time=   2.1s\n",
      "[CV] END max_depth=10, min_samples_split=5, n_estimators=150; total time=   2.1s\n",
      "[CV] END max_depth=10, min_samples_split=10, n_estimators=50; total time=   0.7s\n",
      "[CV] END max_depth=10, min_samples_split=10, n_estimators=50; total time=   0.7s\n",
      "[CV] END max_depth=10, min_samples_split=10, n_estimators=50; total time=   0.7s\n",
      "[CV] END max_depth=10, min_samples_split=10, n_estimators=100; total time=   1.3s\n",
      "[CV] END max_depth=10, min_samples_split=10, n_estimators=100; total time=   1.3s\n",
      "[CV] END max_depth=10, min_samples_split=10, n_estimators=100; total time=   1.3s\n",
      "[CV] END max_depth=10, min_samples_split=10, n_estimators=150; total time=   2.0s\n",
      "[CV] END max_depth=10, min_samples_split=10, n_estimators=150; total time=   2.0s\n",
      "[CV] END max_depth=10, min_samples_split=10, n_estimators=150; total time=   2.0s\n",
      "[CV] END .max_depth=20, min_samples_split=2, n_estimators=50; total time=   0.7s\n",
      "[CV] END .max_depth=20, min_samples_split=2, n_estimators=50; total time=   0.7s\n",
      "[CV] END .max_depth=20, min_samples_split=2, n_estimators=50; total time=   0.7s\n",
      "[CV] END max_depth=20, min_samples_split=2, n_estimators=100; total time=   1.4s\n",
      "[CV] END max_depth=20, min_samples_split=2, n_estimators=100; total time=   1.5s\n",
      "[CV] END max_depth=20, min_samples_split=2, n_estimators=100; total time=   1.4s\n",
      "[CV] END max_depth=20, min_samples_split=2, n_estimators=150; total time=   2.4s\n",
      "[CV] END max_depth=20, min_samples_split=2, n_estimators=150; total time=   2.2s\n",
      "[CV] END max_depth=20, min_samples_split=2, n_estimators=150; total time=   2.2s\n",
      "[CV] END .max_depth=20, min_samples_split=5, n_estimators=50; total time=   0.7s\n",
      "[CV] END .max_depth=20, min_samples_split=5, n_estimators=50; total time=   0.7s\n",
      "[CV] END .max_depth=20, min_samples_split=5, n_estimators=50; total time=   0.7s\n",
      "[CV] END max_depth=20, min_samples_split=5, n_estimators=100; total time=   1.4s\n",
      "[CV] END max_depth=20, min_samples_split=5, n_estimators=100; total time=   1.4s\n",
      "[CV] END max_depth=20, min_samples_split=5, n_estimators=100; total time=   1.4s\n",
      "[CV] END max_depth=20, min_samples_split=5, n_estimators=150; total time=   2.1s\n",
      "[CV] END max_depth=20, min_samples_split=5, n_estimators=150; total time=   2.2s\n",
      "[CV] END max_depth=20, min_samples_split=5, n_estimators=150; total time=   2.1s\n",
      "[CV] END max_depth=20, min_samples_split=10, n_estimators=50; total time=   0.7s\n",
      "[CV] END max_depth=20, min_samples_split=10, n_estimators=50; total time=   0.7s\n",
      "[CV] END max_depth=20, min_samples_split=10, n_estimators=50; total time=   0.7s\n",
      "[CV] END max_depth=20, min_samples_split=10, n_estimators=100; total time=   1.3s\n",
      "[CV] END max_depth=20, min_samples_split=10, n_estimators=100; total time=   1.3s\n",
      "[CV] END max_depth=20, min_samples_split=10, n_estimators=100; total time=   1.3s\n",
      "[CV] END max_depth=20, min_samples_split=10, n_estimators=150; total time=   2.0s\n",
      "[CV] END max_depth=20, min_samples_split=10, n_estimators=150; total time=   2.0s\n",
      "[CV] END max_depth=20, min_samples_split=10, n_estimators=150; total time=   2.0s\n",
      "[CV] END .max_depth=30, min_samples_split=2, n_estimators=50; total time=   0.7s\n",
      "[CV] END .max_depth=30, min_samples_split=2, n_estimators=50; total time=   0.7s\n",
      "[CV] END .max_depth=30, min_samples_split=2, n_estimators=50; total time=   0.7s\n",
      "[CV] END max_depth=30, min_samples_split=2, n_estimators=100; total time=   1.4s\n",
      "[CV] END max_depth=30, min_samples_split=2, n_estimators=100; total time=   1.5s\n",
      "[CV] END max_depth=30, min_samples_split=2, n_estimators=100; total time=   1.4s\n",
      "[CV] END max_depth=30, min_samples_split=2, n_estimators=150; total time=   2.2s\n",
      "[CV] END max_depth=30, min_samples_split=2, n_estimators=150; total time=   2.2s\n",
      "[CV] END max_depth=30, min_samples_split=2, n_estimators=150; total time=   2.2s\n",
      "[CV] END .max_depth=30, min_samples_split=5, n_estimators=50; total time=   0.7s\n",
      "[CV] END .max_depth=30, min_samples_split=5, n_estimators=50; total time=   0.7s\n",
      "[CV] END .max_depth=30, min_samples_split=5, n_estimators=50; total time=   0.7s\n",
      "[CV] END max_depth=30, min_samples_split=5, n_estimators=100; total time=   1.4s\n",
      "[CV] END max_depth=30, min_samples_split=5, n_estimators=100; total time=   1.4s\n",
      "[CV] END max_depth=30, min_samples_split=5, n_estimators=100; total time=   1.4s\n",
      "[CV] END max_depth=30, min_samples_split=5, n_estimators=150; total time=   2.1s\n",
      "[CV] END max_depth=30, min_samples_split=5, n_estimators=150; total time=   2.2s\n",
      "[CV] END max_depth=30, min_samples_split=5, n_estimators=150; total time=   2.3s\n",
      "[CV] END max_depth=30, min_samples_split=10, n_estimators=50; total time=   0.7s\n",
      "[CV] END max_depth=30, min_samples_split=10, n_estimators=50; total time=   0.7s\n",
      "[CV] END max_depth=30, min_samples_split=10, n_estimators=50; total time=   0.7s\n",
      "[CV] END max_depth=30, min_samples_split=10, n_estimators=100; total time=   1.4s\n",
      "[CV] END max_depth=30, min_samples_split=10, n_estimators=100; total time=   1.4s\n",
      "[CV] END max_depth=30, min_samples_split=10, n_estimators=100; total time=   1.4s\n",
      "[CV] END max_depth=30, min_samples_split=10, n_estimators=150; total time=   2.0s\n",
      "[CV] END max_depth=30, min_samples_split=10, n_estimators=150; total time=   2.1s\n",
      "[CV] END max_depth=30, min_samples_split=10, n_estimators=150; total time=   2.0s\n",
      "Best parameters: {'max_depth': 10, 'min_samples_split': 2, 'n_estimators': 50}\n",
      "RMSE: 0.04741342345520828\n",
      "R2 Score: 0.9999952142470777\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from math import sqrt\n",
    "\n",
    "# Load the dataset from a CSV file.\n",
    "# 'sensorcleaned.csv' is the file name, make sure it's in the same directory as the script or provide the full path.\n",
    "data = pd.read_csv('sensorcleaned.csv')\n",
    "\n",
    "# Select features and the target variable from the dataset.\n",
    "# Features are all the columns that might influence the outcome (PM2.5 particle concentration in this case).\n",
    "# The target is what we are trying to predict, here it's 'pm2.5_alt', representing PM2.5 levels.\n",
    "X = data[['humidity', 'temperature', 'pressure', 'voc', '0.3_um_count', '0.5_um_count', '1.0_um_count', '2.5_um_count', '5.0_um_count', '10.0_um_count']]\n",
    "y = data['pm2.5_alt']\n",
    "\n",
    "# Split the dataset into training and testing sets to validate the model performance later.\n",
    "# 80% of the data is used for training and 20% for testing.\n",
    "# Random state is set for reproducibility, ensuring the same split occurs every time the script runs.\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Normalize the feature data using StandardScaler, which removes the mean and scales to unit variance.\n",
    "# This step improves the performance of many machine learning models.\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)  # Fit to data, then transform it.\n",
    "X_test_scaled = scaler.transform(X_test)        # Apply same transformation to test data without fitting.\n",
    "\n",
    "# Initialize the RandomForestRegressor model.\n",
    "# Random forest is a type of ensemble machine learning model which operates by constructing a multitude of decision trees at training time.\n",
    "# Random state is set for reproducibility.\n",
    "model = RandomForestRegressor(random_state=42)\n",
    "\n",
    "# Set up GridSearchCV for hyperparameter tuning.\n",
    "# This will search over specified parameter values for the estimator.\n",
    "# 'n_estimators', 'max_depth', and 'min_samples_split' are parameters of the RandomForest model we are tuning.\n",
    "# GridSearchCV will use cross-validation (cv=3) to evaluate each combination of parameters.\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 150],     # Number of trees in the forest.\n",
    "    'max_depth': [10, 20, 30],          # Maximum depth of the tree.\n",
    "    'min_samples_split': [2, 5, 10]     # Minimum number of samples required to split an internal node.\n",
    "}\n",
    "grid_search = GridSearchCV(estimator=model, param_grid=param_grid, cv=3, scoring='neg_mean_squared_error', verbose=2)\n",
    "grid_search.fit(X_train_scaled, y_train)  # Run grid search to find the best parameters.\n",
    "\n",
    "# Output the best parameters found by GridSearchCV.\n",
    "best_model = grid_search.best_estimator_\n",
    "print(\"Best parameters:\", grid_search.best_params_)\n",
    "\n",
    "# Use the best model to make predictions on the test set.\n",
    "y_pred = best_model.predict(X_test_scaled)\n",
    "\n",
    "# Evaluate the model performance.\n",
    "# RMSE (Root Mean Squared Error) provides an indication of how much error in prediction exists.\n",
    "# R2 Score provides an indication of goodness of fit and should be as close to 1 as possible in a good model.\n",
    "rmse = sqrt(mean_squared_error(y_test, y_pred))  # Calculate RMSE\n",
    "r2 = r2_score(y_test, y_pred)                    # Calculate R2 score\n",
    "print(\"RMSE:\", rmse)\n",
    "print(\"R2 Score:\", r2)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-13T12:03:18.170813Z",
     "start_time": "2024-06-13T12:01:22.901797Z"
    }
   },
   "id": "830975202cfb448d",
   "execution_count": 8
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-13T11:59:01.783196Z",
     "start_time": "2024-06-13T11:59:01.781772Z"
    }
   },
   "id": "1926de5a5ace24f6",
   "execution_count": 7
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
